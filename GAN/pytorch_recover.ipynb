{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b020c13",
   "metadata": {},
   "source": [
    "Intro pytorch  \n",
    "torch index problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a9fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]],\n",
      "\n",
      "        [[9, 0],\n",
      "         [1, 2]]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      " tensor(4) \n",
      " 4\n",
      "tensor([1, 5, 9])\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "example_tensor = torch.tensor([\n",
    "    [[1,2],[3,4]],\n",
    "    [[5,6],[7,8]],\n",
    "    [[9,0],[1,2]]\n",
    "])\n",
    "print(example_tensor)\n",
    "print(example_tensor[0],\"\\n\",\n",
    "      example_tensor[0,1,1],\"\\n\",\n",
    "      example_tensor[0,1,1].item())\n",
    "print(example_tensor[:,0,0])\n",
    "print(example_tensor[1,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ad507",
   "metadata": {},
   "source": [
    "Tensor Properties: Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#看设备\n",
    "print(example_tensor.device)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "#送入gpu\n",
    "example_tensor_gpu = example_tensor.to(device)\n",
    "print(example_tensor_gpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5777dc",
   "metadata": {},
   "source": [
    "Tensor Properties: Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n",
      "torch.Size([3, 2, 2])\n",
      "3\n",
      "2\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "#shape是属性 size()是方法\n",
    "print(example_tensor.shape)\n",
    "print(example_tensor.size())\n",
    "print(example_tensor.shape[0])\n",
    "print(example_tensor.size(1))\n",
    "print(len(example_tensor.shape), len(example_tensor.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fecdf94",
   "metadata": {},
   "source": [
    "Initializing Tensors:  \n",
    "torch.ones_like: creates a tensor of all ones with the same shape and device as example_tensor.  \n",
    "torch.zeros_like: creates a tensor of all zeros with the same shape and device as example_tensor  \n",
    "torch.randn_like: creates a tensor with every element sampled from a Normal (or Gaussian) distribution with the same shape and device as example_tensor  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff93e3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 1],\n",
      "         [1, 1]],\n",
      "\n",
      "        [[1, 1],\n",
      "         [1, 1]],\n",
      "\n",
      "        [[1, 1],\n",
      "         [1, 1]]]) tensor([[0, 0]])\n",
      "tensor([[[-0.3644, -0.4632],\n",
      "         [ 0.4355,  1.0704],\n",
      "         [-0.0189,  1.1808]],\n",
      "\n",
      "        [[-1.1978, -0.5381],\n",
      "         [ 0.9872, -0.6013],\n",
      "         [ 0.7135,  0.3791]],\n",
      "\n",
      "        [[-0.1209,  0.0338],\n",
      "         [ 0.0120, -0.8819],\n",
      "         [ 0.3226,  1.1315]]]) tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.ones_like(example_tensor)\n",
    "tensor0 = torch.zeros_like(torch.tensor([[2,2]]))\n",
    "print(tensor1,tensor0)\n",
    "\n",
    "tensor_radn = torch.randn(3,3,2)\n",
    "tensor_zero = torch.zeros((2,2))\n",
    "print(tensor_radn, tensor_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8e624",
   "metadata": {},
   "source": [
    "Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66193971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -8,  -6],\n",
       "         [ -4,  -2]],\n",
       "\n",
       "        [[  0,   2],\n",
       "         [  4,   6]],\n",
       "\n",
       "        [[  8, -10],\n",
       "         [ -8,  -6]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(example_tensor - 5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07234733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor(4.)\n",
      "Stdev: tensor(2.9848)\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]],\n",
      "\n",
      "        [[9., 0.],\n",
      "         [1., 2.]]])\n",
      "tensor([[5.0000, 2.6667],\n",
      "        [3.6667, 4.6667]])\n",
      "tensor([[2., 3.],\n",
      "        [6., 7.],\n",
      "        [5., 1.]])\n",
      "tensor([[1.5000, 3.5000],\n",
      "        [5.5000, 7.5000],\n",
      "        [4.5000, 1.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean:\", example_tensor.float().mean())\n",
    "print(\"Stdev:\", example_tensor.float().std())\n",
    "print(example_tensor.float())\n",
    "#dim -> 在某一个维度进行压缩\n",
    "print(example_tensor.float().mean(dim=0))\n",
    "print(example_tensor.float().mean(dim=1))\n",
    "print(example_tensor.float().mean(dim=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46820f4",
   "metadata": {},
   "source": [
    "PyTorch Neural Network Module (torch.nn)  \n",
    "nn.BatchNorm1d 是干啥的？   \n",
    "本质：批归一化（Batch Normalization） 技术，作用是让每个批次的输入数据，在训练时保持 一致的均值和标准差 。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0160418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0057,  0.0465],\n",
      "        [-0.0247,  0.1291],\n",
      "        [ 0.1586, -0.1041],\n",
      "        [ 0.1054,  0.2207],\n",
      "        [-0.0208,  0.1896]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0000, 0.0465],\n",
      "        [0.0000, 0.1291],\n",
      "        [0.1586, 0.0000],\n",
      "        [0.1054, 0.2207],\n",
      "        [0.0000, 0.1896]], grad_fn=<ReluBackward0>)\n",
      "tensor([[-0.7893, -0.8461],\n",
      "        [-0.7893,  0.1431],\n",
      "        [ 1.5816, -1.4032],\n",
      "        [ 0.7864,  1.2393],\n",
      "        [-0.7893,  0.8669]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "linear = nn.Linear(10,2)\n",
    "example_input = torch.rand(5,10)\n",
    "example_out = linear(example_input)\n",
    "print(example_out)\n",
    "relu = nn.ReLU()\n",
    "#如果是图像（二维张量），要用 nn.BatchNorm2d ；视频（三维张量）用 nn.BatchNorm3d 。\n",
    "BN = nn.BatchNorm1d(2) #2表示特征的个数\n",
    "print(relu(example_out))\n",
    "print(BN(relu(example_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94facdc",
   "metadata": {},
   "source": [
    "nn.Sequential  \n",
    "nn.Sequential creates a single operation that performs a sequence of operations. For example, you can write a neural network layer with a batch normalization as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c7dbe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0374, 0.0000],\n",
      "        [0.0000, 0.7015],\n",
      "        [0.4990, 0.8359],\n",
      "        [0.0813, 0.9053],\n",
      "        [0.2766, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mlp_layer = nn.Sequential(\n",
    "    nn.Linear(5,2),\n",
    "    nn.BatchNorm1d(2),\n",
    "    nn.ReLU()\n",
    ")\n",
    "test_input = torch.rand(5,5) + 1\n",
    "print(mlp_layer(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be5621",
   "metadata": {},
   "source": [
    "Optimization  \n",
    "To create an optimizer in PyTorch, you'll need to use the torch.optim module, often imported as optim. optim.Adam corresponds to the Adam optimizer.   To create an optimizer object, you'll need to pass it the parameters to be optimized and the learning rate, lr, as well as any other parameters specific to the optimizer.  \n",
    "\n",
    "For all nn objects, you can access their parameters as a list using their parameters() method, as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ba3a75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\YOLO\\lib\\site-packages\\onnxscript\\converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "d:\\Anaconda\\envs\\YOLO\\lib\\site-packages\\onnxscript\\converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "adam_opt = optim.Adam(mlp_layer.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc1b8f",
   "metadata": {},
   "source": [
    "Training Loop  \n",
    "A (basic) training step in PyTorch consists of four basic parts:  \n",
    "\n",
    "1.Set all of the gradients to zero using opt.zero_grad()  \n",
    "2.Calculate the loss, loss  \n",
    "3.Calculate the gradients with respect to the loss using loss.backward()  \n",
    "4.Update the parameters being optimized using opt.step()  \n",
    "That might look like the following code (and you'll notice that if you run it several times, the loss goes down):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13111e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7563, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_example = torch.rand(100,5) + 1\n",
    "adam_opt.zero_grad()\n",
    "#想局部切断用 detach()，想全局禁用梯度用 torch.no_grad() 。\n",
    "cur_loss = torch.abs(1 - mlp_layer(train_example)).mean()\n",
    "cur_loss.backward()\n",
    "adam_opt.step()\n",
    "print(cur_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
